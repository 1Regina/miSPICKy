{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn\n",
    "from sklearn import model_selection, linear_model, svm, naive_bayes, neighbors, ensemble, metrics, datasets\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage.color import *\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import mahotas as mt\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "from PIL import Image\n",
    "# import lightgbm as lgb\n",
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\regina\\desktop\\metis\\metis projects\\spark\\lib\\site-packages (0.90)\n",
      "Requirement already satisfied: numpy in c:\\users\\regina\\desktop\\metis\\metis projects\\spark\\lib\\site-packages (from xgboost) (1.17.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\regina\\desktop\\metis\\metis projects\\spark\\lib\\site-packages (from xgboost) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load images\n",
    "class1 = 'Clean_Room'\n",
    "class2 = 'Messy_Room'\n",
    "class3 = 'Bad_Excuse'\n",
    "path_data = '/Users/regina/Desktop/Metis/Metis Projects/miSPICKy/keras-tutorial - Copy/dataset/'\n",
    "images_class1 = ['/Users/regina/Desktop/Metis/Metis Projects/miSPICKy/keras-tutorial - Copy/dataset/Clean_Room/{}'.format(i) for i in os.listdir(path_data+str(class1)) if str(class1) in i]\n",
    "images_class2 = ['/Users/regina/Desktop/Metis/Metis Projects/miSPICKy/keras-tutorial - Copy/dataset/Messy_Room/{}'.format(i) for i in os.listdir(path_data+str(class2)) if str(class2) in i]\n",
    "images_class3 = ['/Users/regina/Desktop/Metis/Metis Projects/miSPICKy/keras-tutorial - Copy/dataset/Bad_Excuse/{}'.format(i) for i in os.listdir(path_data+str(class2)) if str(class2) in i]\n",
    "# path_data = '/Users/shiheping/Documents/data_science_study/kaplan_metis_bootcamp/projects/project_3/artwork_images/image_training_classes/'\n",
    "# images_class1 = ['/Users/shiheping/Documents/data_science_study/kaplan_metis_bootcamp/projects/project_3/artwork_images/image_training_classes/symbolism/{}'.format(i) for i in os.listdir(path_data+str(class1)) if str(class1) in i]\n",
    "# images_class2 = ['/Users/shiheping/Documents/data_science_study/kaplan_metis_bootcamp/projects/project_3/artwork_images/image_training_classes/baroque/{}'.format(i) for i in os.listdir(path_data+str(class2)) if str(class2) in i]\n",
    "train_class1 = images_class1[:600]\n",
    "train_class2 = images_class2[:600]\n",
    "test_images = images_class1[400:500] + images_class2[400:500]\n",
    "train_images = train_class1 + train_class2\n",
    "random.shuffle(train_images)\n",
    "random.shuffle(test_images)\n",
    "# clear useless list\n",
    "del images_class1\n",
    "del images_class2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 400\n",
    "ncolumns = 400\n",
    "channels = 3\n",
    "\n",
    "def create_features(img):\n",
    "    # read image\n",
    "    #open_image = cv2.imread(img,cv2.IMREAD_COLOR)\n",
    "    open_image = cv2.resize(cv2.imread(img,cv2.IMREAD_COLOR),(nrows,ncolumns),interpolation=cv2.INTER_CUBIC)\n",
    "    # convert to array\n",
    "    img_to_array = np.array(open_image)\n",
    "    # flatten three channel color image\n",
    "    color_features = img_to_array.flatten()\n",
    "    # convert image to greyscale\n",
    "    grey_image = rgb2grey(img_to_array)\n",
    "    # get HOG features from greyscale image\n",
    "    hog_features = hog(grey_image, block_norm='L2-Hys', pixels_per_cell=(16, 16))\n",
    "    # get Haralick (GLCM)\n",
    "    grayscale = cv2.cvtColor(open_image,cv2.COLOR_BGR2GRAY)\n",
    "    textures = mt.features.haralick(grayscale)\n",
    "    ht_mean = textures.mean(axis=0)\n",
    "    # combine color, hog features, Haralick into a single array\n",
    "    flat_features = np.hstack((color_features,hog_features,ht_mean))\n",
    "    return flat_features\n",
    "# Haralick Texture is used to quantify an image based on texture. The fundamental\n",
    "# concept involved in computing Haralick Texture features is the Gray Level Co-occurrence Matrix or GLCM.\n",
    "\n",
    "# Loop through images to preprocess\n",
    "def read_and_create_feature_matrix(list_of_images):\n",
    "    \"\"\"\n",
    "    Returns two arrays:\n",
    "    X is an array of resized images\n",
    "    y is an array of labels\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for image in list_of_images:\n",
    "        image_features = create_features(image)\n",
    "        X.append(image_features)\n",
    "        if class1 in image:\n",
    "            y.append(1)\n",
    "        elif class2 in image:\n",
    "            y.append(0)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(Image.open(train_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape is:  (800, 522862)\n"
     ]
    }
   ],
   "source": [
    "# run create_feature_matrix on our dataframe of images\n",
    "X, y = read_and_create_feature_matrix(train_images)\n",
    "X_test, y_test = read_and_create_feature_matrix(test_images)\n",
    "# Scale feature matrix + PCA\n",
    "# get shape of feature matrix\n",
    "print('Feature matrix shape is: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape is:  (800,)\n",
      "(3, 522862)\n",
      "(1024, 818, 3)\n",
      "(522862,)\n"
     ]
    }
   ],
   "source": [
    "# check function output\n",
    "print('Feature matrix shape is: ', y.shape)\n",
    "type(create_features(train_images[0]))\n",
    "A = []\n",
    "for i in range(3):\n",
    "    a = np.array(create_features(train_images[i]))\n",
    "    A.append(a)\n",
    "print(np.array(A).shape)\n",
    "print(np.array(cv2.imread(train_images[4],cv2.IMREAD_COLOR)).shape)\n",
    "print(create_features(train_images[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA matrix shape is:  (800, 200)\n",
      "PCA matrix shape is:  (200, 200)\n"
     ]
    }
   ],
   "source": [
    "# define standard scaler\n",
    "ss = StandardScaler()\n",
    "# run this on our feature matrix\n",
    "X_scaled = ss.fit_transform(X)\n",
    "X_test_scaled = ss.fit_transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=200)\n",
    "# use fit_transform to run PCA on our standardized matrix\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_test_pca = pca.fit_transform(X_test_scaled)\n",
    "# look at new shape\n",
    "print('PCA matrix shape is: ', X_pca.shape)\n",
    "print('PCA matrix shape is: ', X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cross_validate_evaluate(X_train, y_train, X_test, y_test):\n",
    "    # instantiate classifier\n",
    "    svc = svm.SVC()\n",
    "    rfc = RandomForestClassifier(random_state=seed)\n",
    "    knn = KNeighborsClassifier()\n",
    "    gnb = GaussianNB()\n",
    "    xgb = XGBClassifier()\n",
    "    # GridsearchCV parameters\n",
    "    param_grid_svm = [\n",
    "      {'C': [1, 2, 5, 10, 100], 'kernel': ['linear']},\n",
    "      {'C': [1, 2, 5, 10, 100], 'gamma': [0.5, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n",
    "     {'C': [1, 2, 5, 10, 100], 'gamma': [0.5, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['sigmoid']}]\n",
    "    param_grid_rfc = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,6,8,10],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "    param_grid_knn = {\n",
    "    'n_neighbors': [3,5,11,19],\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']}\n",
    "    param_grid_xgb = {\n",
    "    'reg_alpha': [0.0, 1.0], \n",
    "    'reg_lambda': [0.0, 1.0],\n",
    "    'learning_rate': [0.1,0.01],\n",
    "    'n_estimators': [100, 200], \n",
    "    'seed': [seed]}\n",
    "    # GridSearchCV\n",
    "    score = 'f1_weighted'\n",
    "    clf = GridSearchCV(svc, param_grid_svm, cv=5, scoring=score, n_jobs = -1)\n",
    "    CV_rfc = GridSearchCV(rfc, param_grid=param_grid_rfc, cv= 5, scoring=score, n_jobs = -1)\n",
    "    CV_knn = GridSearchCV(knn, param_grid=param_grid_knn, cv= 5, scoring=score, n_jobs = -1)\n",
    "    CV_xgb = GridSearchCV(xgb, param_grid=param_grid_xgb, cv= 5, scoring=score, n_jobs = -1)\n",
    "    # train\n",
    "    clf.fit(X_train, y_train)\n",
    "    CV_rfc.fit(X_train, y_train)\n",
    "    CV_knn.fit(X_train, y_train)\n",
    "    gnb.fit(X_train,y_train)\n",
    "    CV_xgb.fit(X_train, y_train)\n",
    "    # predict\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(\"Best score: %.1f%%\" % (clf.best_score_*100))\n",
    "    print('SVM:', clf.best_params_)\n",
    "    print(\"Best score: %.1f%%\" % (CV_rfc.best_score_*100))\n",
    "    print('Random Forest:', CV_rfc.best_params_)\n",
    "    print(\"Best score: %.1f%%\" % (CV_knn.best_score_*100))\n",
    "    print('KNN:', CV_knn.best_params_)\n",
    "    print(\"Best score: %.1f%%\" % (CV_xgb.best_score_*100))\n",
    "    print('XGBoost:', CV_xgb.best_params_)\n",
    "    print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print(\"SVM:\")\n",
    "    y_true_svm, y_pred_svm = y_test, clf.best_estimator_.predict(X_test)\n",
    "    print(classification_report(y_true_svm, y_pred_svm))\n",
    "    print()\n",
    "    print(\"Random Forest:\")\n",
    "    y_true_rf, y_pred_rf = y_test, CV_rfc.best_estimator_.predict(X_test)\n",
    "    print(classification_report(y_true_rf, y_pred_rf))\n",
    "    print()\n",
    "    print(\"KNN:\")\n",
    "    y_true_knn, y_pred_knn = y_test, CV_knn.best_estimator_.predict(X_test)\n",
    "    print(classification_report(y_true_knn, y_pred_knn))\n",
    "    print()\n",
    "    print(\"Gaussian Naive Bayes:\")\n",
    "    #y_true_gnb, y_pred_gnb = y_test, gnb.predict(X_test)\n",
    "    print(gnb.score(X_test,y_test))\n",
    "    print()\n",
    "    print(\"XGBoost:\")\n",
    "    y_true_xgb, y_pred_xgb = y_test, CV_xgb.best_estimator_.predict(X_test)\n",
    "    print(classification_report(y_true_xgb, y_pred_xgb))\n",
    "    print()\n",
    "    return clf.best_params_, CV_rfc.best_params_, CV_knn.best_params_, CV_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "Best score: 84.6%\n",
      "SVM: {'C': 1, 'kernel': 'linear'}\n",
      "Best score: 88.6%\n",
      "Random Forest: {'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 200}\n",
      "Best score: 87.4%\n",
      "KNN: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best score: 90.6%\n",
      "KNN: {'learning_rate': 0.1, 'n_estimators': 200, 'reg_alpha': 1.0, 'reg_lambda': 1.0, 'seed': None}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.64      0.67       100\n",
      "           1       0.67      0.74      0.70       100\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.69      0.69      0.69       200\n",
      "weighted avg       0.69      0.69      0.69       200\n",
      "\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       100\n",
      "           1       0.83      0.74      0.78       100\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.80      0.79      0.79       200\n",
      "weighted avg       0.80      0.80      0.79       200\n",
      "\n",
      "\n",
      "KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.57      0.71       100\n",
      "           1       0.69      0.96      0.80       100\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.81      0.76      0.76       200\n",
      "weighted avg       0.81      0.77      0.76       200\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "0.595\n",
      "\n",
      "XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       100\n",
      "           1       0.83      0.81      0.82       100\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.82      0.82      0.82       200\n",
      "weighted avg       0.82      0.82      0.82       200\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_params, rfc_params, knn_params, xgb_params = train_cross_validate_evaluate(X_pca, y, X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 1 2 0 2 0]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_classes = 3\n",
    "b = np.array([0,2,1,1,2,0,2,0])\n",
    "b.reshape(-1) # your initial classes\n",
    "Y = np.eye(num_classes)[b]\n",
    "print(b)\n",
    "print(Y)\n",
    "print(type(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
