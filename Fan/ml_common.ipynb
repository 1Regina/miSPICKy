{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ead5e5847771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_common\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoad_and_Split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.metrics import (classification_report, f1_score, confusion_matrix, \n",
    "                             precision_recall_curve, roc_curve, auc, \n",
    "                             roc_auc_score)\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
    "                                     cross_validate, StratifiedKFold, KFold)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def Vanilla_ML_Run(clf_list, X_train, y_train):\n",
    "    print(\">> Starting %d vanilla ML runs\" %(len(clf_list)))\n",
    "    print()\n",
    "    \n",
    "    new_clf_list = {}\n",
    "    for clf_name, clf in clf_list.items():\n",
    "        print(\" > Training\", clf_name)\n",
    "        clf.fit(X_train, y_train)\n",
    "        new_clf_list[clf_name] = clf\n",
    "    \n",
    "    print()\n",
    "    return new_clf_list\n",
    "\n",
    "\n",
    "def Vanilla_ML_Run_CV(clf_list, X, y, n_splits=5):\n",
    "    print(\">> Starting %d vanilla ML runs\" %(len(clf_list)))\n",
    "    print()\n",
    "    \n",
    "    for clf_name, clf in clf_list.items():\n",
    "        print(\" > Cross-Validation of {} with {} folds\".format(clf_name, n_splits))\n",
    "        skf = StratifiedKFold(n_splits=n_splits)\n",
    "        scores = cross_validate(clf, X, y, cv=skf,\n",
    "                                 scoring=('balanced_accuracy', 'f1_macro', 'f1_weighted',\n",
    "                                          'recall_macro', 'recall_weighted'))\n",
    "    \n",
    "    for score_name, score_value in scores.items():\n",
    "        print(\" >\", score_name)\n",
    "        print(\"   >\", score_value)\n",
    "    print()    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def Predict_and_Report(clf, X_val, y_val, target_names):\n",
    "    y_pred = clf.predict(X_val)\n",
    "    clf_report = classification_report(y_val, y_pred, target_names=target_names)\n",
    "    cf_matrix = confusion_matrix(y_val.argmax(axis=1), \n",
    "                                 y_pred.argmax(axis=1))\n",
    "    print(clf_report)\n",
    "    print(cf_matrix)\n",
    "    print()\n",
    "    return (clf_report, cf_matrix)\n",
    "\n",
    "\n",
    "def Vanilla_ML_Predict(clf_list, X_val, y_val, target_names):\n",
    "    print(\">> Starting %d vanilla ML predictions\" %(len(clf_list)))\n",
    "    print()\n",
    "    \n",
    "    predict_list = {}\n",
    "    for clf_name, clf in clf_list.items():\n",
    "        print(\" > Predicting for\", clf_name)\n",
    "        predict_list[clf_name] = Predict_and_Report(clf, X_val, y_val, target_names)\n",
    "        \n",
    "    print()\n",
    "    return predict_list\n",
    "   \n",
    "\n",
    "def Show_Confusion_Matrix(cf_matrix, target_names, clf_name=\"Model's\"):\n",
    "    \"\"\" Print confusion matrix for specified classifier\n",
    "    \"\"\"\n",
    "    plt.figure(dpi=150)\n",
    "    sns.heatmap(cf_matrix, cmap=plt.cm.Blues, annot=True, square=True,\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "\n",
    "    plt.xlabel('Predicted breeds')\n",
    "    plt.ylabel('Actual breeds')\n",
    "    plt.title('{} Confusion Matrix'.format(clf_name))\n",
    " \n",
    "\n",
    "def Plot_Precision_Recall_Curve(y_test, y_score, target_label, clf_name=\"Model's\"):\n",
    "    \"\"\" Plot precision recall curve for each class onto one chart\n",
    "    \"\"\"\n",
    "    plt.figure(dpi=150)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    for i, label in enumerate(target_label):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                            y_score[:, i])\n",
    "        plt.plot(recall[i], precision[i], lw=2, label='{}'.format(label))\n",
    " \n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"{} Precision vs. Recall Curves\".format(clf_name))\n",
    "    plt.show()\n",
    "    return recall, precision\n",
    "\n",
    "\n",
    "def Plot_ROC_Curve(y_test, y_score, target_label, clf_name=\"Model's\", zoom_level=1.0):\n",
    "    \"\"\" Plot ROC curve for each class onto one chart\n",
    "    \"\"\"\n",
    "    plt.figure(dpi=150)\n",
    "\n",
    "    # Zoom in view of the upper left corner, if zoom_level is set\n",
    "    plt.xlim(0, zoom_level)\n",
    "    plt.ylim(1-zoom_level, 1.05)\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Plot ROC curves for each class\n",
    "    for i, label in enumerate(target_label):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, \n",
    "                 label='{} (AUC: {:.2f})'.format(label, roc_auc[i]))\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    n_classes = y_test.shape[1]\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot both micro and macro average ROC curves\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "            label='Micro-average (AUC: {0:0.2f})'\n",
    "                ''.format(roc_auc[\"micro\"]),\n",
    "                linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "            label='Macro-average (AUC: {0:0.2f})'\n",
    "                ''.format(roc_auc[\"macro\"]),\n",
    "                linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'w--')\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"{} ROC Curves: Breeds vs Averages\".format(clf_name))\n",
    "    plt.show()\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "\n",
    "def Compare_Multiple_ROC_Curves(y_test, y_score, zoom_level=1.0):\n",
    "    \"\"\" Plot multiple ROC curves from different models for comparison\n",
    "    \"\"\"\n",
    "    plt.figure(dpi=150)\n",
    "    # Zoom in view of the upper left corner, if zoom_level is set\n",
    "    plt.xlim(0, zoom_level)\n",
    "    plt.ylim(1-zoom_level, 1.05)\n",
    "\n",
    "    for clf_name in y_test.keys():\n",
    "        fpr, tpr, _ = roc_curve(y_test[clf_name].ravel(), y_score[clf_name].ravel())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=\"{} (AUC: {:.2f})\".format(clf_name, roc_auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'w--')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title(\"ROC Curves of different Models\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Plot_AUC_ROC_Curve_old():\n",
    "    from scipy import interp\n",
    "    import matplotlib.pyplot as plt\n",
    "    from itertools import cycle\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    n_classes = 3\n",
    "\n",
    "    # Plot linewidth.\n",
    "    lw = 2\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(1)\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "            label='Micro-average ROC curve (area = {0:0.2f})'\n",
    "                ''.format(roc_auc[\"micro\"]),\n",
    "            color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "            label='Macro-average ROC curve (area = {0:0.2f})'\n",
    "                ''.format(roc_auc[\"macro\"]),\n",
    "            color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['red', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                label='ROC curve of {0} (area = {1:0.2f})'\n",
    "                ''.format(class_label[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate',fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate',fontweight='bold')\n",
    "    plt.title('Extension of Receiver Operating Characteristic to Multi-class',fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def Plot_AUC_ROC_Curve_ZoomIn():\n",
    "    # Zoom in view of the upper left corner.\n",
    "    plt.figure(2)\n",
    "    plt.xlim(0, 0.2)\n",
    "    plt.ylim(0.8, 1)\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "            label='Micro-average ROC curve (area = {0:0.2f})'\n",
    "                ''.format(roc_auc[\"micro\"]),\n",
    "            color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "            label='Macro-average ROC curve (area = {0:0.2f})'\n",
    "                ''.format(roc_auc[\"macro\"]),\n",
    "            color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['red', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                label='ROC curve of {0} (area = {1:0.2f})'\n",
    "                ''.format(class_label[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlabel('False Positive Rate',fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate',fontweight='bold')\n",
    "    plt.title('Extension of Receiver Operating Characteristic to Multi-class',fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def Save_Model_Data(model, model_name, X_test, y_test, history=''):\n",
    "    with open('models/' + model_name + '_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "            \n",
    "    with open('models/' + model_name + '_x_test_data.csv', 'w') as f:\n",
    "        np.savetxt(f, X_test)\n",
    "    \n",
    "    with open('models/' + model_name + '_y_test_data.csv', 'w') as f:\n",
    "        np.savetxt(f, y_test)\n",
    "\n",
    "    if history:\n",
    "        with open('models/' + model_name + '_history.pkl', 'wb') as f:\n",
    "            pickle.dump(history, f)        \n",
    "            \n",
    "        \n",
    "def Load_Model_Data(model_name, neural_network=False):\n",
    "    model = pickle.load(open('models/' + model_name + '_model.pkl', 'rb'))\n",
    "    X_test = np.loadtxt('models/' + model_name + '_x_test_data.csv')\n",
    "    y_test = np.loadtxt('models/' + model_name + '_y_test_data.csv')\n",
    "    \n",
    "    if neural_network:\n",
    "        history = pickle.load(open('models/' + model_name + '_history.pkl', 'rb'))\n",
    "        return model, X_test, y_test, history\n",
    "    else:\n",
    "        return model, X_test, y_test\n",
    "\n",
    "\n",
    "def Save_Model_Data_old(model, history, X_test, y_test, model_name):\n",
    "    with open('models/' + model_name + '_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    with open('models/' + model_name + '_history.pkl', 'wb') as f:\n",
    "            pickle.dump(history, f)        \n",
    "            \n",
    "    with open('models/' + model_name + '_x_test_data.csv', 'w') as f:\n",
    "        np.savetxt(f, X_test)\n",
    "    \n",
    "    with open('models/' + model_name + '_y_test_data.csv', 'w') as f:\n",
    "        np.savetxt(f, y_test)\n",
    "        \n",
    "        \n",
    "def Load_Model_Data_old(model_name):\n",
    "    model = pickle.load(open('models/' + model_name + '_model.pkl', 'rb'))\n",
    "    history = pickle.load(open('models/' + model_name + '_history.pkl', 'rb'))\n",
    "    X_test = np.loadtxt('models/' + model_name + '_x_test_data.csv')\n",
    "    y_test = np.loadtxt('models/' + model_name + '_y_test_data.csv')\n",
    "    \n",
    "    return model, history, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from lib.data_common import (target_label, Load_and_Split)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    X_train, y_train = Load_and_Split('data/cavy_data_train.csv', (150,150))\n",
    "    X_test, y_test = Load_and_Split('data/cavy_data_test.csv', (150,150))\n",
    "\n",
    "    clf_list = {'log reg': LogisticRegression(multi_class='ovr', n_jobs=-1)}\n",
    "    new_clf_list = Vanilla_ML_Run(clf_list, X_train, y_train)\n",
    " \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
