{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python classify_images.py\n",
    "# python classify_images.py --model svm\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from PIL import Image\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# initialize the data and labels\n",
    "print(\"[INFO] loading images...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "imagePaths = sorted(list(paths.list_images('dataset')))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "    # load the image, resize the image to be 32x32 pixels (ignoring\n",
    "    # aspect ratio), flatten the image into 32x32x3=3072 pixel image\n",
    "    # into a list, and store the image in the data list\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.resize(image, (32, 32)).flatten() #keep in mind this resize dimension for predict later\n",
    "    data.append(image)\n",
    " \n",
    "    # extract the class label from the image path and update the labels list\n",
    "    # label according to the folder\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(label)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the raw pixelintensities to the range [0, 1] for each class\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01176471, 0.03529412, 0.06666667, ..., 0.4627451 , 0.4627451 ,\n",
       "        0.48235294],\n",
       "       [0.0627451 , 0.15686275, 0.2627451 , ..., 0.05098039, 0.1372549 ,\n",
       "        0.2627451 ],\n",
       "       [0.58039216, 0.69411765, 0.78039216, ..., 0.57254902, 0.57254902,\n",
       "        0.57254902],\n",
       "       ...,\n",
       "       [0.56078431, 0.58039216, 0.57647059, ..., 0.16470588, 0.17254902,\n",
       "        0.23137255],\n",
       "       [0.00392157, 0.00392157, 0.00392157, ..., 0.        , 0.01176471,\n",
       "        0.01176471],\n",
       "       [0.26666667, 0.35686275, 0.44313725, ..., 0.15686275, 0.18823529,\n",
       "        0.18431373]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bad_Excuse', 'Bad_Excuse', 'Clean_Room', ..., 'Clean_Room',\n",
       "       'Bad_Excuse', 'Bad_Excuse'], dtype='<U10')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into 75% training and 25% validation\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42745098, 0.53333333, 0.67843137, ..., 0.49411765, 0.76470588,\n",
       "        0.82352941],\n",
       "       [0.23529412, 0.58039216, 0.87843137, ..., 0.38823529, 0.50980392,\n",
       "        0.63921569],\n",
       "       [0.6745098 , 0.64705882, 0.81960784, ..., 0.43137255, 0.56078431,\n",
       "        0.7372549 ],\n",
       "       ...,\n",
       "       [0.23137255, 0.21960784, 0.22745098, ..., 0.03137255, 0.03921569,\n",
       "        0.07058824],\n",
       "       [0.97254902, 0.97254902, 0.97254902, ..., 0.94509804, 0.93333333,\n",
       "        0.94117647],\n",
       "       [0.28627451, 0.30196078, 0.43921569, ..., 0.05490196, 0.00392157,\n",
       "        0.00392157]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Messy_Room', 'Clean_Room', 'Clean_Room', ..., 'Bad_Excuse',\n",
       "       'Messy_Room', 'Messy_Room'], dtype='<U10')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dictionary of models our script can use, where the key\n",
    "# to the dictionary is the name of the model (supplied via command\n",
    "# line argument) and the value is the model itself\n",
    "models = {\n",
    "\t\"knn\": KNeighborsClassifier(n_neighbors=1),\n",
    "\t\"naive_bayes\": GaussianNB(),\n",
    "\t\"logit\": LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "\t\"svm\": SVC(kernel=\"linear\"),\n",
    "\t\"decision_tree\": DecisionTreeClassifier(),\n",
    "\t\"random_forest\": RandomForestClassifier(n_estimators=100),\n",
    "\t\"mlp\": MLPClassifier()\n",
    "# \t\"xgb\": XGBClassifier()    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dictionary of models our script can use, where the key\n",
    "# to the dictionary is the name of the model (supplied via command\n",
    "# line argument) and the value is the model itself\n",
    "models = {\n",
    "\t\"knn\": KNeighborsClassifier(n_neighbors=1),\n",
    "\t\"naive_bayes\": GaussianNB(),\n",
    "\t\"logit\": LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "\t\"svm\": SVC(kernel=\"linear\"),\n",
    "\t\"decision_tree\": DecisionTreeClassifier(),\n",
    "\t\"random_forest\": RandomForestClassifier(n_estimators=100),\n",
    "\t\"mlp\": MLPClassifier()\n",
    "# \t\"xgb\": XGBClassifier()    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels, converting them from strings to integers\n",
    "# vector format for each class (0,1,2)\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'XGBoost' model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(\"XGBoost\"))\n",
    "model = XGBClassifier()\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating XGBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Bad_Excuse       0.87      0.91      0.89       160\n",
      "  Clean_Room       0.74      0.76      0.75       236\n",
      "  Messy_Room       0.70      0.64      0.67       162\n",
      "\n",
      "    accuracy                           0.77       558\n",
      "   macro avg       0.77      0.77      0.77       558\n",
      "weighted avg       0.76      0.77      0.76       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating XGBoostClassifier\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "\n",
    "# import the necessary packages\n",
    "import cv2\n",
    "# load the input image and resize it to the target spatial dimensions\n",
    "width = 32 # remember the input dimension used to resize the train data\n",
    "height = 32 # remember the input dimension used to resize the train data\n",
    "image = cv2.imread(\"images\\SGStove.jpg\")\n",
    "output = image.copy()\n",
    "image = cv2.resize(image, (width, height)).flatten() #the train data were flattened so predict image must be flatten too\n",
    "\n",
    "# scale the pixel values to [0, 1]\n",
    "# data = np.array(data, dtype=\"float\") / 255.0\n",
    "image = np.array(image, dtype=\"float\") / 255.0\n",
    "image = image.reshape((1, image.shape[0]))\n",
    "\n",
    "# # make a prediction on the image\n",
    "# preds = model.predict(image)\n",
    "# proba = model.predict_proba(image)\n",
    "\n",
    "# print (preds)\n",
    "# print (proba)\n",
    "# # zip(model.classes_, model.predict_proba(image))\n",
    "# # # find the class label index with the largest corresponding probability\n",
    "# # # i = preds.argmax(axis=1)[0]\n",
    "# # # label = le.classes_[i]\n",
    "\n",
    "# # draw the class label + probability on the output image\n",
    "# text = \"{}: {:.2f}%\".format(label, preds[0][i] * 100)\n",
    "# cv2.putText(output, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "# # show the output image\n",
    "# cv2.imshow(\"Image\", output)\n",
    "# cv2.waitKey(0)   # Delay in milliseconds. 0 is the special value that means “forever”, until you close the image window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0037402, 0.6213763, 0.3748835]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(image)\n",
    "# make a prediction on the image\n",
    "preds = model.predict(image)\n",
    "preds\n",
    "proba = model.predict_proba(image)\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = [[0,1,1,0],[1,1,1,0]]\n",
    "pd.DataFrame(clf.predict_proba(test), columns=clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'knn' model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(\"knn\"))\n",
    "model = models[\"knn\"]\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating KNeighborsClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Bad_Excuse       0.36      0.99      0.53       160\n",
      "  Clean_Room       0.57      0.17      0.26       236\n",
      "  Messy_Room       0.91      0.26      0.40       162\n",
      "\n",
      "    accuracy                           0.43       558\n",
      "   macro avg       0.61      0.47      0.40       558\n",
      "weighted avg       0.61      0.43      0.38       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating KNeighborsClassifier\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'naive_bayes' model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(\"naive_bayes\"))\n",
    "model = models[\"naive_bayes\"]\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating GaussianNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Bad_Excuse       0.74      0.78      0.76       160\n",
      "  Clean_Room       0.64      0.53      0.58       236\n",
      "  Messy_Room       0.55      0.66      0.60       162\n",
      "\n",
      "    accuracy                           0.64       558\n",
      "   macro avg       0.64      0.66      0.65       558\n",
      "weighted avg       0.64      0.64      0.64       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating GaussianNB\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'logit' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\regina\\Desktop\\Metis\\Metis Projects\\Spark\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(\"logit\"))\n",
    "model = models[\"logit\"]\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Bad_Excuse       0.61      0.75      0.67       160\n",
      "  Clean_Room       0.59      0.60      0.60       236\n",
      "  Messy_Room       0.62      0.46      0.53       162\n",
      "\n",
      "    accuracy                           0.60       558\n",
      "   macro avg       0.61      0.60      0.60       558\n",
      "weighted avg       0.61      0.60      0.60       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating LogisticRegression\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'svm' model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(\"svm\"))\n",
    "model = models[\"svm\"]\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Bad_Excuse       0.76      0.74      0.75       160\n",
      "  Clean_Room       0.59      0.56      0.57       236\n",
      "  Messy_Room       0.53      0.59      0.56       162\n",
      "\n",
      "    accuracy                           0.62       558\n",
      "   macro avg       0.63      0.63      0.63       558\n",
      "weighted avg       0.62      0.62      0.62       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating SVC\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'decision_tree' model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(\"decision_tree\"))\n",
    "model = models[\"decision_tree\"]\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Bad_Excuse       0.76      0.74      0.75       160\n",
      "  Clean_Room       0.59      0.56      0.57       236\n",
      "  Messy_Room       0.53      0.59      0.56       162\n",
      "\n",
      "    accuracy                           0.62       558\n",
      "   macro avg       0.63      0.63      0.63       558\n",
      "weighted avg       0.62      0.62      0.62       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating DecisionTree\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'random_forest' model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(\"random_forest\"))\n",
    "model = models[\"random_forest\"]\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Bad_Excuse       0.89      0.88      0.88       160\n",
      "  Clean_Room       0.69      0.78      0.74       236\n",
      "  Messy_Room       0.70      0.57      0.63       162\n",
      "\n",
      "    accuracy                           0.75       558\n",
      "   macro avg       0.76      0.74      0.75       558\n",
      "weighted avg       0.75      0.75      0.75       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating RandomForest\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'mlp' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\regina\\Desktop\\Metis\\Metis Projects\\Spark\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(\"mlp\"))\n",
    "model = models[\"mlp\"]\n",
    "model.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Bad_Excuse       0.60      0.91      0.73       160\n",
      "  Clean_Room       0.68      0.47      0.56       236\n",
      "  Messy_Room       0.66      0.62      0.64       162\n",
      "\n",
      "    accuracy                           0.64       558\n",
      "   macro avg       0.65      0.67      0.64       558\n",
      "weighted avg       0.65      0.64      0.63       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating MLPClassifier\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
